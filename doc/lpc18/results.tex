\section{Experimental results}\label{sec:results}
\subsection{Testbed}
All of our performance results use a hardware testbed that consists of
two Intel Xeon E5 2440 v2 1.9GHz servers, each with an Intel 10GbE X540-AT2 dual
port NIC, with the two ports of the Intel NIC on one server connected
to the two ports on the identical NIC on the other server.i
We installed p4c-xdp on one server, the {\em target server}, and
attached the XDP program to the port that receives the packets
The other server, the {\em source server}, generates packets
at the maximum 10~Gbps packet rate of 14.88~Mpps using the DPDK-based
TRex~\cite{trex} traffic generator.  The source server sends minimum
length 64-byte packets in {\em single} UDP flow to one port of the
target server, and receives the forwarded packets on the same port.
At the target server, every packet received goes through the
pipeline specified in P4.

We use sample p4 programs under the tests directory and the following
metrics to understand the performance impact of the P4-generated XDP
program:
\begin{itemize}
\item Packet Processing Rate (Mpps): Once XDP program finishes processing
the packet, it returns one of the actions mentioned in section~\ref{background}.
We made a small modification to all p4 program to always return XDP\_DROP,
so that we can count the number of packets being drop per second as a
indication of how fast the XDP can process.
\item CPU Utilization: Every packets processed by XDP program is run
under the per-core software IRQ daemon, named ksoftirqd/<core id>.
We measure the CPU utilization of the ksoftirqd on the core.
\item Number of BPF instructions verified: For each program, we list
the its max complexity; the total number of BPF instructions the
verifier has to go through, as an indication of how complicated the
program is.
\end{itemize}

The target server is running Linux kernel 4.19-rc5 and for all our
tests, the BPF JIT (Just-In-Time) compiler is enabled and JIT harden
is disabled. All programs are compiled with clang 3.8 with llvm 5.0.
For For each test program, we use the following
command from iproute2 tool to load it into kernel:
%\begin{verbatim}
\texttt{ip link set dev eth0 xdp obj xdp1.o verb}.
%\end{verbatim}

The Intel 10GbE X540 NIC is running ixgbe driver with 16 RX queues
set-up. Since the source server is sending single UDP flow, packets
always arrive at a single ring ID.  As a result, we collect the number
of packets being dropped at this ring.

\subsection{Results}
To compare the performance, we first started by manually writing two
XDP programs. The first one, SimpleDrop, does nothing but drop all packets by
returning XDP\_DROP. The second program also does nothing but returns
XDP\_TX, which forwards the packet to the receiving port.  Both programs
consists of only two BPF instructions.

{\small
\begin{verbatim}
    /* SimpleDrop */
    0: (b7) r0 = 1 // XDP_DROP
    1: (95) exit

    /* SimpleTX */
    0: (b7) r0 = 3 // XDP_TX
    1: (95) exit
\end{verbatim}
}
Then We attached the following P4 programs to the device receiving the
rate of 14.88~Mpps to evaluate the overhead introduce by the P4C-XDP
compiler.
\begin{itemize}
\item xdp1.p4: Parse Ethernet/IPv4 header, deparse it, and drop.
\item xdp3.p4: Parse Ethernet/IPv4 header, lookup an mac address
table, deparse it, and drop.
\item xdp6.p4: Parse Ethernet/IPv4 header, lookup and get a new TTL value
from eBPF map, set to IPv4 header, deparse it, and drop.
\item xdp7.p4: Parse Ethernet/IPv4/UDP header, write a pre-defined source port
and source IP, recalculate checksum, deparse, and drop.
\item xdp11.p4: Parse Ethernet/IPv4 header, swap src/dst mac address,
deparse it, and send back to the same port (XDP\_TX).
\item xdp15.p4: Parse Ethernet header, insert a customized 8-byte header,
deparse it, and send back to the same port (XDP\_TX).
\end{itemize}

\begin{table}
\centering
\small
\begin{tabular}{llll}
  \underline{P4 program} & \underline{CPU Util.} & \underline{Mpps} & \underline{Insns./Stack}\\
  SimpleDROP & 75\% & 14.4 & 2/0 \\
  SimpleTX & 100\% & 7.2 & 2/0 \\
  xdp1.p4 &  100\% &  8.1 & 277/256 \\
  xdp3.p4 &  100\% &  7.1 & 326/256 \\
  xdp6.p4 &  100\% &  2.5 & 335/272 \\
  xdp7.p4 &  100\% &  5.7 & 5821/336 \\
  xdp11.p4 &  100\% &  4.7  & 335/216 \\
  xdp15.p4 &  100\% &  5.5 & 96/56\\
\end{tabular}
\caption{\footnotesize Performance of XDP program generated by
  p4c-xdp compiler.}
\label{tab:perf}
\end{table}

As shown in Table~\ref{tab:perf}, the xdp1.p4 shows the baseline overhead
introduced by adding the parser and deparser, dropping the rate from 14.4~Mpps to
8.1~Mpps. The xdp3.p4 drops another million packet per second due to
calling the eBPF map lookup function to do a lookup, the lookup is designed
to always return NULL so no value from the map is accessed.
The xdp6.p4 shows significant overhead because it is designed to lookup
a table, find a new TTL value, and write to the IPv4 header. 
Surprisingly, the xdp7.p4 does extra parsing to the UDP header and
checksum recalculation, but the overhead is moderate due to not accessing
the table.

Finally, the xdp11.p4 and xdp15.p4 shows the transmit (XDP\_TX) performance.
Compared with xdp11 and xdp15, the xdp15.p4 involves the extra bpf\_adjust\_head helper
function to reset the pointer for extra bytes.  
Interestingly, it does not incur much over head because there is
already a reserved space in front of every XDP packet frame.

\subsection{Microbenchmark}
To further understand the performance overhead of programs generated by p4c-xdp,
we manually {\emph comments out} the entire deparser portion of the C code, so that
we can identify which stage of our XDP program (parser, lookup, and deparser) incurs
the performance overhead.

\begin{table}
\centering
\small
\begin{tabular}{llll}
  \underline{P4 program} & \underline{CPU Util.} & \underline{Mpps} & \underline{Insns./Stack}\\
  xdp1.p4 &  77\% &  14.8 & 26/0 \\
  xdp3.p4 &  100\% &  13 & 100/16 \\
  xdp6.p4 &  100\% &  12 & 98/40 \\
\end{tabular}
\caption{\footnotesize Performance of XDP program without deparser.}
\label{tab:perf2}
\end{table}

As shown in Table~\ref{tab:perf2}, the performance increases significantly.
By investigating our p4c-xdp compiler implementation and the generated C
code, we figured out that the deparser is unconditionally writing back
the entire packet content even when the P4 program does not modify any.
In addition, the deparser incurs lots of byte-order translation, e.g.,
htonl, ntohl. This could be avoided by always using network byte-order
in P4 and XDP. We leave this for future optimization.
