\section{Experimental results}\label{sec:results}

\subsection{Testbed}
All of our performance results use a hardware testbed that consists of
two Intel Xeon E5 2440 servers, each with an Intel 10GbE X540-AT2 dual
port NIC, with the two ports of the Intel NIC on one server connected
to the two ports on the identical NIC on the other server.i
We installed p4c-xdp on one server, the {\em target server}, and
attached the XDP program to the port that receives the packets
The other server, the {\em source server}, generates packets
at the maximum 10~Gbps packet rate of 14.88~Mpps using the DPDK-based
TRex~\cite{trex} traffic generator.  The source server sends minimum
length 64-byte packets in {\em single} UDP flow to one port of the
target server, and receives the forwarded packets on the same port.
At the target server, every packet received goes through the
pipeline specified in P4.

We use sample p4 programs under the tests directory and the following
metrics to understand the performance impact of the P4-generated XDP
program:
\begin{itemize}
\item CPU Utilization: Every packets processed by XDP program is run
under the ksoftirqd, the kernel's software IRQ daemon.
We measure the CPU utilization of the ksoftirqd.
\item Packet processing rate: Once XDP program finishes processing the
packet, it returns one of the action mentioned in section XXX.
We made a small modification to all p4 program to return XDP\_DROP,
and count the number of packets being drop per second as a indication
of how fast the XDP can process.
\item Number of BPF instructions verified: For each program, we list
the its max complexity, the total number of BPF instructions the
verifier has to go through, as an indication of how complicated the
program is.
\end{itemize}

The target server is running Linux kernel 4.19-rc2 and for all our
tests, the BPF JIT (Just-In-Time) compiler is enabled. 
For each test program, we use the following
command from iproute2 tool to load it into kernel:
\begin{verbatim}
ip link set dev eth0 xdp obj xdp1.o verb
\end{verbatim}
The Intel 10GbE X540 NIC is running ixgbe driver with 16 RX queues
set-up. Since the source server is sending single UDP flow, packets
always arrive at a single ring ID.  As a result, we collect the number
of packets being dropped at this ring.

\subsection{Results}
To compare the performance, we first started by manually writing two
XDP program. The first one simply does nothing but drop all packets by
returning XDP\_DROP. The second program also does nothing but returns
XDP\_TX, which forwards the packet to the receiving port.  Both programs
consists of only two BPF instructions.
0: (b7) r0 = 1
1: (95) exit

Then We attached the following P4 programs to the device receiving the
rate of 14.88~Mpps to evaluate the overhead introduce by the P4C-XDP
compiler.
\begin{itemize}
\item xdp1.p4: Parse Ethernet/IPv4 header, deparse it, and drop.
\item xdp3.p4: Parse Ethernet/IPv4 header, lookup an mac address
table, deparse it, and drop.
\item xdp6.p4: Parse Ethernet/IPv4 header, lookup and get an 
ttl from metadata, set to IPv4 header, deparse it, and drop.
\item xdp7.p4: Parse Ethernet/IPv4/UDP header, modify source port
and source IP, recalculate checksum, deparse, and drop.
\item xdp11.p4: Parse Ethernet/IPv4 header, swap src/dst mac address,
deparse it, and send back to the same port (XDP\_TX).
\item xdp15.p4: Parse Ethernet header, insert a customized 8-byte header,
deparse it, and send back to the same port (XDP\_TX).
\end{itemize}


As show in Table X, the xdp1.p4 shows the baseline overhead introduced
by adding the parser and deparser, dropping the rate from 14.4~Mpps to
8.1~Mpps. The xdp3.p4 drops another million packet per second due to
calling the eBPF map lookup function to do a lookup.
The xdp6.p4 shows significant overhead because it (why?) 
Surprisingly, the xdp7.p4 does extra parsing to the UDP header and
checksum recalculation, but the overhead is moderate.

Finally, the xdp11.p4 and xdp15.p4 tests the XDP\_TX performance.
xdp15.p4 involves the bpf\_adjust\_head helper function.
Interestingly, it does not incur much over head because there is
already a reserved space in front of every packet frame.

\begin{table}
\centering
\small
\begin{tabular}{lll}
  \underline{P4 program} & \underline{CPU Util.} & \underline{Mpps} \\
  SimpleDROP & 75\% & 14.4 \\
  SimpleTX & WIP & WIP \\
  xdp1.p4 &  100\% &  8.1 \\
  xdp3.p4 &  100\% &  7.1 \\
  xdp6.p4 &  100\% &  2.5 \\
  xdp7.p4 &  100\% &  5.7 \\
  xdp11.p4 &  100\% &  4.7 \\
  xdp15.p4 &  100\% &  5.5 \\
\end{tabular}
\caption{\footnotesize performance of XDP program generated by
  p4c-xdp compiler.}
\label{table:treebuild}
\end{table}
